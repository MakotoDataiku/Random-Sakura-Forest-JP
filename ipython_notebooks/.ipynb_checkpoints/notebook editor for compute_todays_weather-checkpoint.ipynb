{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_code = {\"東京\":{\"prec_no\":\"44\", \"block_no\":\"47662\", \"type\":\"s1\"}, \"大分\":{\"prec_no\":\"83\", \"block_no\":\"0808\", \"type\":\"a1\"}, \"弘前\":{\"prec_no\":\"31\", \"block_no\":\"0166\", \"type\":\"a1\"}}\n",
    "base_url = \"http://www.data.jma.go.jp/obd/stats/etrn/view/daily_%s.php?prec_no=%s&block_no=%s&year=%s&month=%s&day=&view=p1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2float(str):\n",
    "    try:\n",
    "        return float(str)\n",
    "    except:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "#都市を網羅します\n",
    "for place in place_code.keys():\n",
    "    #最終的にデータを集めるリスト (下に書いてある初期値は一行目。つまり、ヘッダー。)\n",
    "    print(place)\n",
    "    # index = place_name.index(place)\n",
    "    prec_no = place_code[place][\"prec_no\"]\n",
    "    block_no = place_code[place][\"block_no\"]\n",
    "    data_type = place_code[place][\"type\"]\n",
    "\n",
    "    year = 2021\n",
    "    # その年の1月~12月の12回を網羅する。\n",
    "    for month in range(1,13):\n",
    "        #print(month)\n",
    "        #2つの都市コードと年と月を当てはめる。\n",
    "        r = requests.get(base_url%(data_type, prec_no, block_no, year, month))\n",
    "        r.encoding = r.apparent_encoding\n",
    "\n",
    "        # まずはサイトごとスクレイピング\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        # findAllで条件に一致するものをすべて抜き出します。\n",
    "        # 今回の条件はtrタグでclassがmtxになってるものです。\n",
    "        rows = soup.findAll('tr',class_='mtx')\n",
    "\n",
    "        # 表の最初の1~4行目はカラム情報なのでスライスする。(indexだから初めは0だよ)\n",
    "        # 【追記】2020/3/11 申し訳ございません。間違えてました。\n",
    "        if data_type == \"a1\":\n",
    "            rows_data = rows[3:]\n",
    "\n",
    "            # 1日〜最終日までの１行を網羅し、取得します。\n",
    "            for row in rows_data:\n",
    "                # 今度はtrのなかのtdをすべて抜き出します\n",
    "                data = row.findAll('td')\n",
    "\n",
    "                #１行の中には様々なデータがあるので全部取り出す。\n",
    "                # ★ポイント\n",
    "                day = data[0].findAll('a')[0].string # 日にち\n",
    "                date = str(year) + \"/\" + str(month) + \"/\" + day\n",
    "                precip_total  = data[1].string #降水量(mm)合計\n",
    "                temp_ave = data[4].string #平均気温\n",
    "                temp_high = data[5].string #最高気温\n",
    "                temp_low = data[6].string #最低気温\n",
    "                daylight = data[13].string #日照時間\n",
    "                d = {\"date\":date, \"precip_total\":precip_total, \"temp_ave\":temp_ave, \"temp_high\":temp_high, \"temp_low\":temp_low, \"daylight\":daylight, \"place\":place}\n",
    "                l.append(d)\n",
    "        else:\n",
    "            rows_data = rows[4:]\n",
    "            for row in rows_data:\n",
    "                # 今度はtrのなかのtdをすべて抜き出します\n",
    "                data = row.findAll('td')\n",
    "\n",
    "                #１行の中には様々なデータがあるので全部取り出す。\n",
    "                # ★ポイント\n",
    "                day = data[0].findAll('a')[0].string\n",
    "                date = str(year) + \"/\" + str(month) + \"/\" + day\n",
    "                precip_total  = data[3].string #降水量(mm)合計\n",
    "                temp_ave = data[6].string #平均気温\n",
    "                temp_high = data[7].string #最高気温\n",
    "                temp_low = data[8].string #最低気温\n",
    "                daylight = data[16].string #日照時間\n",
    "                d = {\"date\":date, \"precip_total\":precip_total, \"temp_ave\":temp_ave, \"temp_high\":temp_high, \"temp_low\":temp_low, \"daylight\":daylight, \"place\":place}\n",
    "                l.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(l)\n",
    "\n",
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "# Write recipe outputs\n",
    "tokyo_kyushu_temp = dataiku.Dataset(\"tokyo_kyushu_temp\")\n",
    "tokyo_kyushu_temp.write_with_schema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute recipe outputs\n",
    "# TODO: Write here your actual code that computes the outputs\n",
    "# NB: DSS supports several kinds of APIs for reading and writing data. Please see doc.\n",
    "\n",
    "todays_weather_df = ... # Compute a Pandas dataframe to write into todays_weather\n",
    "\n",
    "\n",
    "# Write recipe outputs\n",
    "todays_weather = dataiku.Dataset(\"todays_weather\")\n",
    "todays_weather.write_with_schema(todays_weather_df)"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_todays_weather",
  "creator": "mmiyazaki",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python (env ebay-scraping__2_)",
   "language": "python",
   "name": "py-dku-venv-ebay-scraping__2_"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
